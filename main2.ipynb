{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repositório com o código desenvolvido para realizar o trabalho (https://github.com/VictorHenrique317/ml-projeto-final) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install xgboost\n",
    "# %pip install seaborn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from IPython.core.display import Image\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost as xgb\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo os diferentes conjuntos de features.\n",
    "- X_questions são as perguntas feitas para o paciente durante a consulta.\n",
    "- X_drugs são os remédios que o paciente toma.\n",
    "- X é a junção de X_Questions e X_drugs.\n",
    "- X_random é um conjunto de features aleatórias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfold: 1\n",
      "     0.00%... | Accuracy on batch 1:  56.25%\n",
      "     3.23%... | Accuracy on batch 2:  59.38%\n",
      "     9.68%... | Accuracy on batch 4:  59.38%\n",
      "     16.13%... | Accuracy on batch 6:  59.38%\n",
      "     22.58%... | Accuracy on batch 8:  59.38%\n",
      "     29.03%... | Accuracy on batch 10:  59.38%\n",
      "     32.26%... | Accuracy on batch 11:  59.38%\n",
      "     38.71%... | Accuracy on batch 13:  59.38%\n",
      "     41.94%... | Accuracy on batch 14:  59.38%\n",
      "     45.16%... | Accuracy on batch 15:  62.50%\n",
      "     48.39%... | Accuracy on batch 16:  62.50%\n",
      "     51.61%... | Accuracy on batch 17:  62.50%\n",
      "     58.06%... | Accuracy on batch 19:  62.50%\n",
      "     64.52%... | Accuracy on batch 21:  62.50%\n",
      "     70.97%... | Accuracy on batch 23:  62.50%\n",
      "     77.42%... | Accuracy on batch 25:  62.50%\n",
      "     83.87%... | Accuracy on batch 27:  62.50%\n",
      "     90.32%... | Accuracy on batch 29:  62.50%\n",
      "     96.77%... | Accuracy on batch 31:  62.50%\n",
      "kfold: 2\n",
      "     0.00%... | Accuracy on batch 1:  58.06%\n",
      "     6.45%... | Accuracy on batch 3:  58.06%\n",
      "     12.90%... | Accuracy on batch 5:  58.06%\n",
      "     19.35%... | Accuracy on batch 7:  58.06%\n",
      "     25.81%... | Accuracy on batch 9:  58.06%\n",
      "     32.26%... | Accuracy on batch 11:  58.06%\n",
      "     35.48%... | Accuracy on batch 12:  64.52%\n",
      "     41.94%... | Accuracy on batch 14:  64.52%\n",
      "     48.39%... | Accuracy on batch 16:  64.52%\n",
      "     51.61%... | Accuracy on batch 17:  64.52%\n",
      "     58.06%... | Accuracy on batch 19:  64.52%\n",
      "     64.52%... | Accuracy on batch 21:  64.52%\n",
      "     70.97%... | Accuracy on batch 23:  64.52%\n",
      "     77.42%... | Accuracy on batch 25:  64.52%\n",
      "     83.87%... | Accuracy on batch 27:  64.52%\n",
      "     90.32%... | Accuracy on batch 29:  64.52%\n",
      "     96.77%... | Accuracy on batch 31:  64.52%\n",
      "kfold: 3\n",
      "     0.00%... | Accuracy on batch 1:  45.16%\n",
      "     3.23%... | Accuracy on batch 2:  51.61%\n",
      "     9.68%... | Accuracy on batch 4:  51.61%\n",
      "     16.13%... | Accuracy on batch 6:  51.61%\n",
      "     22.58%... | Accuracy on batch 8:  51.61%\n",
      "     29.03%... | Accuracy on batch 10:  51.61%\n",
      "     35.48%... | Accuracy on batch 12:  51.61%\n",
      "     38.71%... | Accuracy on batch 13:  51.61%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 251\u001b[0m\n\u001b[1;32m    246\u001b[0m clusters[\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m old_value0\n\u001b[1;32m    248\u001b[0m clf \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBClassifier(max_depth\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m,  gamma\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, eta\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, min_child_weight\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, subsample\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m, \n\u001b[1;32m    249\u001b[0m                             colsample_bytree\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m, scale_pos_weight\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 251\u001b[0m clf \u001b[39m=\u001b[39m trainClassifier(X_labeled, y_labeled, clusters, clf, \u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[84], line 71\u001b[0m, in \u001b[0;36mtrainClassifier\u001b[0;34m(X_labeled, y_labeled, clusters, clf, print_accuracy)\u001b[0m\n\u001b[1;32m     69\u001b[0m last_accuracy \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, repeats):\n\u001b[0;32m---> 71\u001b[0m     clf\u001b[39m.\u001b[39;49mfit(X_train, y_train) \u001b[39m# classificador generico\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     accuracy \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mscore(X_test, y_test)\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m accuracy \u001b[39m<\u001b[39m last_accuracy:\n\u001b[1;32m     75\u001b[0m         \u001b[39m# remove the last batch from X_train and y_train\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1462\u001b[0m (\n\u001b[1;32m   1463\u001b[0m     model,\n\u001b[1;32m   1464\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1470\u001b[0m )\n\u001b[1;32m   1471\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1472\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[1;32m   1473\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1487\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[1;32m   1488\u001b[0m )\n\u001b[0;32m-> 1490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1491\u001b[0m     params,\n\u001b[1;32m   1492\u001b[0m     train_dmatrix,\n\u001b[1;32m   1493\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1494\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1495\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1496\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1497\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1498\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1499\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1500\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1501\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1502\u001b[0m )\n\u001b[1;32m   1504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[1;32m   1505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[1;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[1;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[1;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def defineUnlabeledPoints(X_true, y_true, clusters):\n",
    "    # cosine 55%\n",
    "    # l2 57%\n",
    "    y_true = y_true.reset_index(drop=True)\n",
    "    metric='euclidean'\n",
    "    # Achando os negativos mais proximos\n",
    "    n_neighbors = len(clusters[0])\n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors, algorithm='auto', metric=metric)\n",
    "    nn.fit(clusters[0])\n",
    "\n",
    "    true_negatives_indices = y_true.index[(y_true == 0).all(axis=1)]\n",
    "    true_negatives = X_true.iloc[true_negatives_indices, :]\n",
    "    true_negatives = true_negatives.mean(axis=0).values.reshape(1, -1)\n",
    "\n",
    "    distances, indices = nn.kneighbors(true_negatives)\n",
    "    distances_mean = sum(distances[0])/len(distances[0])\n",
    "    mask = distances < distances_mean\n",
    "    indices = indices[mask]\n",
    "    nearest_negatives = clusters[0].iloc[indices.ravel(), :]\n",
    "\n",
    "    # Achando os positivos mais proximos\n",
    "    n_neighbors = len(clusters[1])\n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors, algorithm='auto', metric=metric)\n",
    "    nn.fit(clusters[1])\n",
    "\n",
    "    true_positives_indices = y_true.index[(y_true == 1).all(axis=1)]\n",
    "    true_positives = X_true.iloc[true_positives_indices, :]\n",
    "    true_positives = true_positives.mean(axis=0).values.reshape(1, -1)\n",
    "\n",
    "    distances, indices = nn.kneighbors(true_positives)\n",
    "    distances_mean = sum(distances[0])/len(distances[0])\n",
    "    mask = distances < distances_mean\n",
    "    indices = indices[mask]\n",
    "    nearest_positives = clusters[1].iloc[indices.ravel(), :]\n",
    "\n",
    "    X_unlabeled = pd.DataFrame(nearest_negatives.iloc[0:1, :])\n",
    "    X_unlabeled = np.concatenate((X_unlabeled, nearest_positives.iloc[0:1, :]), axis=0)\n",
    "    min_length = min(len(nearest_negatives), len(nearest_positives))\n",
    "\n",
    "    for i in range(1, min_length):\n",
    "        X_unlabeled = np.concatenate((X_unlabeled, nearest_negatives.iloc[i: i+1, :]), axis=0)\n",
    "        X_unlabeled = np.concatenate((X_unlabeled, nearest_positives.iloc[i: i+1, :]), axis=0)\n",
    "\n",
    "    if len(nearest_negatives) > min_length:\n",
    "        X_unlabeled = np.concatenate((X_unlabeled, nearest_negatives.iloc[min_length:, :]), axis=0)\n",
    "    elif len(nearest_positives) > min_length:\n",
    "        X_unlabeled = np.concatenate((X_unlabeled, nearest_positives.iloc[min_length:, :]), axis=0)\n",
    "\n",
    "    X_unlabeled = pd.DataFrame(X_unlabeled)\n",
    "    return X_unlabeled\n",
    "\n",
    "def trainClassifier(X_labeled, y_labeled, clusters, clf, print_accuracy=False):\n",
    "    kf = KFold(n_splits=5)\n",
    "    empirical_losses = []\n",
    "    test_losses = []\n",
    "    empirical_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for i, (train_indices, test_indicies) in enumerate(kf.split(X_labeled)):\n",
    "        print(f\"kfold: {i+1}\")\n",
    "        X_train, X_test = X_labeled.iloc[train_indices], X_labeled.iloc[test_indicies]\n",
    "        y_train, y_test = y_labeled.iloc[train_indices], y_labeled.iloc[test_indicies]\n",
    "        unlabeled_points = defineUnlabeledPoints(X_train, y_train, clusters)\n",
    "        \n",
    "        batch_size = 10\n",
    "        repeats = unlabeled_points.shape[0] // batch_size\n",
    "        last_accuracy = 0.0\n",
    "        for i in range(0, repeats):\n",
    "            clf.fit(X_train, y_train) # classificador generico\n",
    "            accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "            if accuracy < last_accuracy:\n",
    "                # remove the last batch from X_train and y_train\n",
    "                rows = X_train.shape[0]\n",
    "                X_train = X_train.iloc[0:rows - batch_size, :]\n",
    "                y_train = y_train.iloc[0:rows - batch_size, :]\n",
    "                continue\n",
    "            \n",
    "            last_accuracy = accuracy\n",
    "\n",
    "            print(f\"    {i/repeats*100: .2f}%... | Accuracy on batch {i + 1}: {accuracy * 100: .2f}%\")\n",
    "\n",
    "            delta_X_batch = pd.DataFrame(unlabeled_points.iloc[i*batch_size : batch_size*(i+1), :])\n",
    "            delta_y_batch = pd.DataFrame(clf.predict(delta_X_batch))\n",
    "\n",
    "            X_train = pd.concat([X_train, delta_X_batch], axis=0)\n",
    "            y_train = pd.concat([y_train, delta_y_batch], axis=0)\n",
    "\n",
    "        empirical_loss = log_loss(y_train, clf.predict(X_train))\n",
    "        test_loss = log_loss(y_test, clf.predict(X_test))\n",
    "\n",
    "        empirical_accuracy = clf.score(X_train, y_train)\n",
    "        test_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "        empirical_losses.append(empirical_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        empirical_accuracies.append(empirical_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "    empirical_loss = np.mean(empirical_losses)\n",
    "    test_loss = np.mean(test_losses)\n",
    "\n",
    "    empirical_accuracy = np.mean(empirical_accuracies) * 100\n",
    "    test_accuracy = np.mean(test_accuracies) * 100\n",
    "\n",
    "    if print_accuracy:\n",
    "        print()\n",
    "        print(f\"empirical_accuracy: {empirical_accuracy: .2f}% | test_accuracy: {test_accuracy: .2f}% \")\n",
    "\n",
    "    return (empirical_loss, test_loss)\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "X_questions = data.iloc[:, 2:182] # Id e data de nascimento são irrelevantes\n",
    "X_questions = X_questions.drop('date_visit', axis=1) # Data de visita não é relevante\n",
    "X_questions = X_questions.drop(X_questions.columns[[46, 133, 158, 161]], axis=1) # Essas colunas são constantes\n",
    "\n",
    "X_drugs = data.iloc[:, 185:]\n",
    "X_drugs = X_drugs.drop(X_drugs.columns[[50,51,61,92,101,111,114,121,137,140,141,\n",
    "                                        142,143,148,151,152]], axis=1) # Essas colunas são constantes\n",
    "\n",
    "X_random = np.random.rand(X_questions.shape[0], 1) # Para comparar a perfomance do modelo\n",
    "\n",
    "# Codificando as variáveis categóricas\n",
    "le = LabelEncoder()\n",
    "for col in X_questions.columns:\n",
    "    if X_questions[col].dtype == 'bool':\n",
    "        X_questions[col] = le.fit_transform(X_questions[col])\n",
    "\n",
    "for col in X_drugs.columns:\n",
    "    if X_drugs[col].dtype == 'bool':\n",
    "        X_drugs[col] = le.fit_transform(X_drugs[col])\n",
    "\n",
    "# Imputando os valores que faltam\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "imp.fit(X_questions)\n",
    "X_questions = imp.transform(X_questions)\n",
    "\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "imp.fit(X_drugs)\n",
    "X_drugs = imp.transform(X_drugs)\n",
    "\n",
    "# Normalizando os dados\n",
    "scaler = StandardScaler()\n",
    "X_drugs = scaler.fit_transform(X_drugs)\n",
    "X_questions = scaler.fit_transform(X_questions)\n",
    "X_random = scaler.fit_transform(X_random)\n",
    "\n",
    "X_questions = pd.DataFrame(X_questions)\n",
    "X_drugs = pd.DataFrame(X_drugs)\n",
    "X_random = pd.DataFrame(X_random)\n",
    "\n",
    "Y = data.iloc[:, 182:185]\n",
    "\n",
    "y_vas30 = Y.iloc[:, 0:1].values.ravel()\n",
    "y_vas50 = Y.iloc[:, 1:2].values.ravel()\n",
    "y_gic = Y.iloc[:, 2:3].values.ravel()\n",
    "\n",
    "y_perceived = np.logical_and(y_vas30, y_vas50)\n",
    "y_perceived = y_perceived.astype(int)\n",
    "\n",
    "y = np.logical_and(y_perceived, y_gic)\n",
    "y = y.astype(int)\n",
    "y = pd.DataFrame(y)\n",
    "    \n",
    "one_rows = pd.DataFrame(y.index[(y == 1).all(axis=1)]) # Achando os indices das linhas que tem y=1\n",
    "zero_rows = pd.DataFrame( y.index[(y == 0).all(axis=1)]) # Achando os indices das linhas que tem y=0\n",
    "zero_rows = zero_rows.sample(n=one_rows.shape[0], random_state=42)\n",
    "\n",
    "# LABELED DATA\n",
    "X_questions_labeled = X_questions.iloc[one_rows[0].tolist() + zero_rows[0].tolist(), :].astype(int)\n",
    "X_drugs_labeled = X_drugs.iloc[one_rows[0].tolist() + zero_rows[0].tolist(), :].astype(int)\n",
    "X_labeled = np.concatenate((X_questions_labeled, X_drugs_labeled), axis=1).astype(int) # A junção das duas tabelas\n",
    "\n",
    "X_questions_labeled = pd.DataFrame(X_questions_labeled)\n",
    "X_drugs_labeled = pd.DataFrame(X_drugs_labeled)\n",
    "X_labeled = pd.DataFrame(X_labeled)\n",
    "\n",
    "y_labeled = y.iloc[one_rows[0].tolist() + zero_rows[0].tolist(), :].astype(int)\n",
    "y_labeled = pd.DataFrame(y_labeled)\n",
    "\n",
    "# UNLABELED DATA\n",
    "X_questions_unlabeled = X_questions.drop(index=X_questions_labeled.index)\n",
    "X_drugs_unlabeled = X_drugs.drop(index=X_drugs_labeled.index)\n",
    "X_unlabeled = np.concatenate((X_questions_unlabeled, X_drugs_unlabeled), axis=1) # A junção das duas tabelas\n",
    "y_unlabeled = y.drop(index=y_labeled.index)\n",
    "\n",
    "X_questions_unlabeled = pd.DataFrame(X_questions_unlabeled)\n",
    "X_drugs_unlabeled = pd.DataFrame(X_drugs_unlabeled)\n",
    "X_unlabeled = pd.DataFrame(X_unlabeled)\n",
    "y_unlabeled = pd.DataFrame(y_unlabeled)\n",
    "\n",
    "# SHUFFLE\n",
    "X_questions_labeled = X_questions_labeled.reset_index(drop=True)\n",
    "X_drugs_labeled = X_drugs_labeled.reset_index(drop=True)\n",
    "X_labeled = X_labeled.reset_index(drop=True)\n",
    "y_labeled = y_labeled.reset_index(drop=True)\n",
    "\n",
    "random_state = 42\n",
    "indices = y_labeled.index.tolist()\n",
    "shuffled_indices = pd.DataFrame(indices).sample(frac=1, random_state=random_state).values.ravel()\n",
    "X_questions_labeled = X_questions_labeled.iloc[shuffled_indices, :]\n",
    "X_drugs_labeled = X_drugs_labeled.iloc[shuffled_indices, :]\n",
    "X_labeled = X_labeled.iloc[shuffled_indices, :]\n",
    "y_labeled = y_labeled.iloc[shuffled_indices, :]\n",
    "\n",
    "# random_state = 42 # set a random state for reproducibility\n",
    "# indices = X_questions_labeled.index.tolist() # get the indices of the dataframes\n",
    "# shuffled_indices = pd.DataFrame(indices).sample(frac=1, random_state=random_state).values.ravel() # shuffle the indices\n",
    "\n",
    "# X_questions_labeled = X_questions_labeled.loc[shuffled_indices, :] # sort X_questions_labeled by the shuffled indices\n",
    "# X_drugs_labeled = X_drugs_labeled.loc[shuffled_indices, :] # sort X_drugs_labeled by the shuffled indices\n",
    "# X_labeled = X_labeled.loc[shuffled_indices, :] # sort X_labeled by the shuffled indices\n",
    "# y_labeled = y_labeled.loc[shuffled_indices, :] # sort y_labeled by the shuffled indices\n",
    "\n",
    "n_clusters = 2\n",
    "# model = AgglomerativeClustering(n_clusters=n_clusters, distance_threshold=None)\n",
    "# model.fit(X_unlabeled)\n",
    "\n",
    "# Z = linkage(X_unlabeled, method='ward')\n",
    "# plt.figure(figsize=(10, 5)) # plot the dendrogram\n",
    "# dendrogram(Z, truncate_mode='lastp', p=n_clusters)\n",
    "# # plt.show()\n",
    "\n",
    "# labels = model.labels_\n",
    "# unique_labels = np.unique(labels)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=100, max_iter=300)\n",
    "labels = kmeans.fit_predict(X_unlabeled)\n",
    "clusters = {}\n",
    "for i, label in enumerate(labels):\n",
    "    if label not in clusters:\n",
    "        clusters[label] = []\n",
    "    clusters[label].append(X_unlabeled.iloc[i:i+1, :])\n",
    "    # clusters[label] = clusters[label].append(X_unlabeled.iloc[i:i+1, :])\n",
    "    \n",
    "for key in clusters.keys():\n",
    "    clusters[key] = pd.concat(clusters[key])\n",
    "\n",
    "old_value0 = clusters[0]\n",
    "old_value1 = clusters[1]\n",
    "\n",
    "clusters[0] = old_value1\n",
    "clusters[1] = old_value0\n",
    "\n",
    "clf = xgb.XGBClassifier(max_depth=7,  gamma=0.7, eta=0.01, min_child_weight=1, subsample=0.8, \n",
    "                            colsample_bytree=0.8, scale_pos_weight=1)\n",
    "\n",
    "clf = trainClassifier(X_labeled, y_labeled, clusters, clf, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
