{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repositório com o código desenvolvido para realizar o trabalho (https://github.com/VictorHenrique317/ml-projeto-final) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install xgboost\n",
    "# %pip install seaborn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from IPython.core.display import Image\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost as xgb\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo os diferentes conjuntos de features.\n",
    "- X_questions são as perguntas feitas para o paciente durante a consulta.\n",
    "- X_drugs são os remédios que o paciente toma.\n",
    "- X é a junção de X_Questions e X_drugs.\n",
    "- X_random é um conjunto de features aleatórias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[480, 90, 255, 323, 487, 168, 267, 47, 159, 373, 258, 424, 173, 392, 446, 0, 111, 147, 268, 525, 410, 349, 339, 117, 49, 523, 314, 107, 9, 442, 41, 133, 260, 246, 413, 232, 179, 132, 187, 481, 67, 450, 484, 453, 34, 463, 401, 291, 458, 292, 242, 434, 224, 516, 218, 172, 65, 62, 245, 214, 414, 521, 31, 362, 189, 237, 374, 532, 500, 396, 375, 518, 241, 146, 511, 68, 124, 160]\n",
      "kfold: 1\n",
      "Train indices: [ 32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49\n",
      "  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67\n",
      "  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85\n",
      "  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103\n",
      " 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121\n",
      " 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139\n",
      " 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155]\n",
      "Test indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31]\n",
      "     0.00%... | Accuracy on batch 1:  53.12%\n",
      "     3.23%... | Accuracy on batch 2:  53.12%\n",
      "     6.45%... | Accuracy on batch 3:  53.12%\n",
      "     9.68%... | Accuracy on batch 4:  56.25%\n",
      "     16.13%... | Accuracy on batch 6:  56.25%\n",
      "     19.35%... | Accuracy on batch 7:  59.38%\n",
      "     25.81%... | Accuracy on batch 9:  59.38%\n",
      "     32.26%... | Accuracy on batch 11:  59.38%\n",
      "     38.71%... | Accuracy on batch 13:  59.38%\n",
      "     45.16%... | Accuracy on batch 15:  59.38%\n",
      "     51.61%... | Accuracy on batch 17:  59.38%\n",
      "     58.06%... | Accuracy on batch 19:  59.38%\n",
      "     64.52%... | Accuracy on batch 21:  59.38%\n",
      "     70.97%... | Accuracy on batch 23:  59.38%\n",
      "     77.42%... | Accuracy on batch 25:  59.38%\n",
      "     83.87%... | Accuracy on batch 27:  59.38%\n",
      "     90.32%... | Accuracy on batch 29:  59.38%\n",
      "     96.77%... | Accuracy on batch 31:  59.38%\n",
      "kfold: 2\n",
      "Train indices: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  63  64  65  66\n",
      "  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84\n",
      "  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102\n",
      " 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120\n",
      " 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138\n",
      " 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155]\n",
      "Test indices: [32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55\n",
      " 56 57 58 59 60 61 62]\n",
      "     0.00%... | Accuracy on batch 1:  45.16%\n",
      "     6.45%... | Accuracy on batch 3:  45.16%\n",
      "     9.68%... | Accuracy on batch 4:  45.16%\n",
      "     16.13%... | Accuracy on batch 6:  45.16%\n",
      "     19.35%... | Accuracy on batch 7:  45.16%\n",
      "     22.58%... | Accuracy on batch 8:  45.16%\n",
      "     25.81%... | Accuracy on batch 9:  45.16%\n",
      "     29.03%... | Accuracy on batch 10:  48.39%\n",
      "     35.48%... | Accuracy on batch 12:  48.39%\n",
      "     41.94%... | Accuracy on batch 14:  48.39%\n",
      "     48.39%... | Accuracy on batch 16:  48.39%\n",
      "     51.61%... | Accuracy on batch 17:  48.39%\n",
      "     54.84%... | Accuracy on batch 18:  48.39%\n",
      "     61.29%... | Accuracy on batch 20:  48.39%\n",
      "     64.52%... | Accuracy on batch 21:  48.39%\n",
      "     67.74%... | Accuracy on batch 22:  48.39%\n",
      "     74.19%... | Accuracy on batch 24:  48.39%\n",
      "     80.65%... | Accuracy on batch 26:  48.39%\n",
      "     87.10%... | Accuracy on batch 28:  48.39%\n",
      "     93.55%... | Accuracy on batch 30:  48.39%\n",
      "kfold: 3\n",
      "Train indices: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  94  95  96  97  98  99 100 101 102\n",
      " 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120\n",
      " 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138\n",
      " 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155]\n",
      "Test indices: [63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86\n",
      " 87 88 89 90 91 92 93]\n",
      "     0.00%... | Accuracy on batch 1:  54.84%\n",
      "     3.23%... | Accuracy on batch 2:  61.29%\n",
      "     6.45%... | Accuracy on batch 3:  61.29%\n",
      "     12.90%... | Accuracy on batch 5:  61.29%\n",
      "     19.35%... | Accuracy on batch 7:  61.29%\n",
      "     25.81%... | Accuracy on batch 9:  61.29%\n",
      "     32.26%... | Accuracy on batch 11:  61.29%\n",
      "     38.71%... | Accuracy on batch 13:  61.29%\n",
      "     45.16%... | Accuracy on batch 15:  61.29%\n",
      "     51.61%... | Accuracy on batch 17:  61.29%\n",
      "     58.06%... | Accuracy on batch 19:  61.29%\n",
      "     64.52%... | Accuracy on batch 21:  61.29%\n",
      "     70.97%... | Accuracy on batch 23:  61.29%\n",
      "     77.42%... | Accuracy on batch 25:  61.29%\n",
      "     83.87%... | Accuracy on batch 27:  61.29%\n",
      "     90.32%... | Accuracy on batch 29:  61.29%\n",
      "     96.77%... | Accuracy on batch 31:  61.29%\n",
      "kfold: 4\n",
      "Train indices: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93 125 126 127 128 129 130 131 132 133 134 135 136 137 138\n",
      " 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155]\n",
      "Test indices: [ 94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111\n",
      " 112 113 114 115 116 117 118 119 120 121 122 123 124]\n",
      "     0.00%... | Accuracy on batch 1:  64.52%\n",
      "     3.23%... | Accuracy on batch 2:  64.52%\n",
      "     6.45%... | Accuracy on batch 3:  64.52%\n",
      "     9.68%... | Accuracy on batch 4:  67.74%\n",
      "     12.90%... | Accuracy on batch 5:  67.74%\n",
      "     16.13%... | Accuracy on batch 6:  74.19%\n",
      "     22.58%... | Accuracy on batch 8:  74.19%\n",
      "     29.03%... | Accuracy on batch 10:  74.19%\n",
      "     35.48%... | Accuracy on batch 12:  74.19%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 254\u001b[0m\n\u001b[0;32m    249\u001b[0m clusters[\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m old_value0\n\u001b[0;32m    251\u001b[0m clf \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBClassifier(max_depth\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m,  gamma\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, eta\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, min_child_weight\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, subsample\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m, \n\u001b[0;32m    252\u001b[0m                             colsample_bytree\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m, scale_pos_weight\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 254\u001b[0m clf \u001b[39m=\u001b[39m trainClassifier(X_labeled, y_labeled, clusters, clf, \u001b[39m5\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn[16], line 73\u001b[0m, in \u001b[0;36mtrainClassifier\u001b[1;34m(X_labeled, y_labeled, clusters, clf, k, print_accuracy)\u001b[0m\n\u001b[0;32m     71\u001b[0m last_accuracy \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     72\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, repeats):\n\u001b[1;32m---> 73\u001b[0m     clf\u001b[39m.\u001b[39;49mfit(X_train, y_train) \u001b[39m# classificador generico\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     accuracy \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mscore(X_test, y_test)\n\u001b[0;32m     76\u001b[0m     \u001b[39mif\u001b[39;00m accuracy \u001b[39m<\u001b[39m last_accuracy:\n\u001b[0;32m     77\u001b[0m         \u001b[39m# remove the last batch from X_train and y_train\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:1471\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1460\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mnum_class\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_\n\u001b[0;32m   1462\u001b[0m (\n\u001b[0;32m   1463\u001b[0m     model,\n\u001b[0;32m   1464\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1470\u001b[0m )\n\u001b[1;32m-> 1471\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1472\u001b[0m     missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[0;32m   1473\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   1474\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m   1475\u001b[0m     group\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1476\u001b[0m     qid\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1477\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1478\u001b[0m     base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[0;32m   1479\u001b[0m     feature_weights\u001b[39m=\u001b[39;49mfeature_weights,\n\u001b[0;32m   1480\u001b[0m     eval_set\u001b[39m=\u001b[39;49meval_set,\n\u001b[0;32m   1481\u001b[0m     sample_weight_eval_set\u001b[39m=\u001b[39;49msample_weight_eval_set,\n\u001b[0;32m   1482\u001b[0m     base_margin_eval_set\u001b[39m=\u001b[39;49mbase_margin_eval_set,\n\u001b[0;32m   1483\u001b[0m     eval_group\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1484\u001b[0m     eval_qid\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1485\u001b[0m     create_dmatrix\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dmatrix,\n\u001b[0;32m   1486\u001b[0m     enable_categorical\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menable_categorical,\n\u001b[0;32m   1487\u001b[0m     feature_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_types,\n\u001b[0;32m   1488\u001b[0m )\n\u001b[0;32m   1490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1491\u001b[0m     params,\n\u001b[0;32m   1492\u001b[0m     train_dmatrix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1501\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[0;32m   1502\u001b[0m )\n\u001b[0;32m   1504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:448\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[1;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_evaluation_matrices\u001b[39m(\n\u001b[0;32m    429\u001b[0m     missing: \u001b[39mfloat\u001b[39m,\n\u001b[0;32m    430\u001b[0m     X: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    444\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[0;32m    445\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[39mstr\u001b[39m]]]:\n\u001b[0;32m    446\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[39m    way.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 448\u001b[0m     train_dmatrix \u001b[39m=\u001b[39m create_dmatrix(\n\u001b[0;32m    449\u001b[0m         data\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    450\u001b[0m         label\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    451\u001b[0m         group\u001b[39m=\u001b[39;49mgroup,\n\u001b[0;32m    452\u001b[0m         qid\u001b[39m=\u001b[39;49mqid,\n\u001b[0;32m    453\u001b[0m         weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    454\u001b[0m         base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[0;32m    455\u001b[0m         feature_weights\u001b[39m=\u001b[39;49mfeature_weights,\n\u001b[0;32m    456\u001b[0m         missing\u001b[39m=\u001b[39;49mmissing,\n\u001b[0;32m    457\u001b[0m         enable_categorical\u001b[39m=\u001b[39;49menable_categorical,\n\u001b[0;32m    458\u001b[0m         feature_types\u001b[39m=\u001b[39;49mfeature_types,\n\u001b[0;32m    459\u001b[0m         ref\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    460\u001b[0m     )\n\u001b[0;32m    462\u001b[0m     n_validation \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m eval_set \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(eval_set)\n\u001b[0;32m    464\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Sequence:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:908\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[1;34m(self, ref, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[0;32m    907\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m--> 908\u001b[0m \u001b[39mreturn\u001b[39;00m DMatrix(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, nthread\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:743\u001b[0m, in \u001b[0;36mDMatrix.__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical)\u001b[0m\n\u001b[0;32m    740\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    741\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 743\u001b[0m handle, feature_names, feature_types \u001b[39m=\u001b[39m dispatch_data_backend(\n\u001b[0;32m    744\u001b[0m     data,\n\u001b[0;32m    745\u001b[0m     missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[0;32m    746\u001b[0m     threads\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnthread,\n\u001b[0;32m    747\u001b[0m     feature_names\u001b[39m=\u001b[39;49mfeature_names,\n\u001b[0;32m    748\u001b[0m     feature_types\u001b[39m=\u001b[39;49mfeature_types,\n\u001b[0;32m    749\u001b[0m     enable_categorical\u001b[39m=\u001b[39;49menable_categorical,\n\u001b[0;32m    750\u001b[0m )\n\u001b[0;32m    751\u001b[0m \u001b[39massert\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39m=\u001b[39m handle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\data.py:970\u001b[0m, in \u001b[0;36mdispatch_data_backend\u001b[1;34m(data, missing, threads, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_tuple(data, missing, threads, feature_names, feature_types)\n\u001b[0;32m    969\u001b[0m \u001b[39mif\u001b[39;00m _is_pandas_df(data):\n\u001b[1;32m--> 970\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_pandas_df(data, enable_categorical, missing, threads,\n\u001b[0;32m    971\u001b[0m                            feature_names, feature_types)\n\u001b[0;32m    972\u001b[0m \u001b[39mif\u001b[39;00m _is_pandas_series(data):\n\u001b[0;32m    973\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_pandas_series(\n\u001b[0;32m    974\u001b[0m         data, missing, threads, enable_categorical, feature_names, feature_types\n\u001b[0;32m    975\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\data.py:420\u001b[0m, in \u001b[0;36m_from_pandas_df\u001b[1;34m(data, enable_categorical, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_from_pandas_df\u001b[39m(\n\u001b[0;32m    410\u001b[0m     data: DataFrame,\n\u001b[0;32m    411\u001b[0m     enable_categorical: \u001b[39mbool\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    415\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[0;32m    416\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DispatchedDataBackendReturnType:\n\u001b[0;32m    417\u001b[0m     data, feature_names, feature_types \u001b[39m=\u001b[39m _transform_pandas_df(\n\u001b[0;32m    418\u001b[0m         data, enable_categorical, feature_names, feature_types\n\u001b[0;32m    419\u001b[0m     )\n\u001b[1;32m--> 420\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_numpy_array(data, missing, nthread, feature_names, feature_types)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\data.py:214\u001b[0m, in \u001b[0;36m_from_numpy_array\u001b[1;34m(data, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[0;32m    208\u001b[0m args \u001b[39m=\u001b[39m {\n\u001b[0;32m    209\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmissing\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(missing),\n\u001b[0;32m    210\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mnthread\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mint\u001b[39m(nthread),\n\u001b[0;32m    211\u001b[0m }\n\u001b[0;32m    212\u001b[0m config \u001b[39m=\u001b[39m \u001b[39mbytes\u001b[39m(json\u001b[39m.\u001b[39mdumps(args), \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    213\u001b[0m _check_call(\n\u001b[1;32m--> 214\u001b[0m     _LIB\u001b[39m.\u001b[39;49mXGDMatrixCreateFromDense(\n\u001b[0;32m    215\u001b[0m         _array_interface(data),\n\u001b[0;32m    216\u001b[0m         config,\n\u001b[0;32m    217\u001b[0m         ctypes\u001b[39m.\u001b[39;49mbyref(handle),\n\u001b[0;32m    218\u001b[0m     )\n\u001b[0;32m    219\u001b[0m )\n\u001b[0;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m handle, feature_names, feature_types\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def defineUnlabeledPoints(X_true, y_true, clusters):\n",
    "    # cosine 55%\n",
    "    # l2 57%\n",
    "    y_true = y_true.reset_index(drop=True)\n",
    "    metric='euclidean'\n",
    "    # Achando os negativos mais proximos\n",
    "    n_neighbors = len(clusters[0])\n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors, algorithm='auto', metric=metric)\n",
    "    nn.fit(clusters[0])\n",
    "\n",
    "    true_negatives_indices = y_true.index[(y_true == 0).all(axis=1)]\n",
    "    true_negatives = X_true.iloc[true_negatives_indices, :]\n",
    "    true_negatives = true_negatives.mean(axis=0).values.reshape(1, -1)\n",
    "\n",
    "    distances, indices = nn.kneighbors(true_negatives)\n",
    "    distances_mean = sum(distances[0])/len(distances[0])\n",
    "    mask = distances < distances_mean\n",
    "    indices = indices[mask]\n",
    "    nearest_negatives = clusters[0].iloc[indices.ravel(), :]\n",
    "\n",
    "    # Achando os positivos mais proximos\n",
    "    n_neighbors = len(clusters[1])\n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors, algorithm='auto', metric=metric)\n",
    "    nn.fit(clusters[1])\n",
    "\n",
    "    true_positives_indices = y_true.index[(y_true == 1).all(axis=1)]\n",
    "    true_positives = X_true.iloc[true_positives_indices, :]\n",
    "    true_positives = true_positives.mean(axis=0).values.reshape(1, -1)\n",
    "\n",
    "    distances, indices = nn.kneighbors(true_positives)\n",
    "    distances_mean = sum(distances[0])/len(distances[0])\n",
    "    mask = distances < distances_mean\n",
    "    indices = indices[mask]\n",
    "    nearest_positives = clusters[1].iloc[indices.ravel(), :]\n",
    "\n",
    "    X_unlabeled = pd.DataFrame(nearest_negatives.iloc[0:1, :])\n",
    "    X_unlabeled = np.concatenate((X_unlabeled, nearest_positives.iloc[0:1, :]), axis=0)\n",
    "    min_length = min(len(nearest_negatives), len(nearest_positives))\n",
    "\n",
    "    for i in range(1, min_length):\n",
    "        X_unlabeled = np.concatenate((X_unlabeled, nearest_negatives.iloc[i: i+1, :]), axis=0)\n",
    "        X_unlabeled = np.concatenate((X_unlabeled, nearest_positives.iloc[i: i+1, :]), axis=0)\n",
    "\n",
    "    if len(nearest_negatives) > min_length:\n",
    "        X_unlabeled = np.concatenate((X_unlabeled, nearest_negatives.iloc[min_length:, :]), axis=0)\n",
    "    elif len(nearest_positives) > min_length:\n",
    "        X_unlabeled = np.concatenate((X_unlabeled, nearest_positives.iloc[min_length:, :]), axis=0)\n",
    "\n",
    "    X_unlabeled = pd.DataFrame(X_unlabeled)\n",
    "    return X_unlabeled\n",
    "\n",
    "def trainClassifier(X_labeled, y_labeled, clusters, clf, k, print_accuracy=False):\n",
    "    kf = KFold(n_splits=k)\n",
    "    empirical_losses = []\n",
    "    test_losses = []\n",
    "    empirical_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for i, (train_indices, test_indicies) in enumerate(kf.split(X_labeled)):\n",
    "        print(f\"kfold: {i+1}\")\n",
    "        print(f\"Train indices: {train_indices}\")\n",
    "        print(f\"Test indices: {test_indicies}\")\n",
    "        X_train, X_test = X_labeled.iloc[train_indices], X_labeled.iloc[test_indicies]\n",
    "        y_train, y_test = y_labeled.iloc[train_indices], y_labeled.iloc[test_indicies]\n",
    "        unlabeled_points = defineUnlabeledPoints(X_train, y_train, clusters)\n",
    "        \n",
    "        batch_size = 10\n",
    "        repeats = unlabeled_points.shape[0] // batch_size\n",
    "        last_accuracy = 0.0\n",
    "        for i in range(0, repeats):\n",
    "            clf.fit(X_train, y_train) # classificador generico\n",
    "            accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "            if accuracy < last_accuracy:\n",
    "                # remove the last batch from X_train and y_train\n",
    "                rows = X_train.shape[0]\n",
    "                X_train = X_train.iloc[0:rows - batch_size, :]\n",
    "                y_train = y_train.iloc[0:rows - batch_size, :]\n",
    "                continue\n",
    "            \n",
    "            last_accuracy = accuracy\n",
    "\n",
    "            print(f\"    {i/repeats*100: .2f}%... | Accuracy on batch {i + 1}: {accuracy * 100: .2f}%\")\n",
    "\n",
    "            delta_X_batch = pd.DataFrame(unlabeled_points.iloc[i*batch_size : batch_size*(i+1), :])\n",
    "            delta_y_batch = pd.DataFrame(clf.predict(delta_X_batch))\n",
    "\n",
    "            X_train = pd.concat([X_train, delta_X_batch], axis=0)\n",
    "            y_train = pd.concat([y_train, delta_y_batch], axis=0)\n",
    "\n",
    "        empirical_loss = log_loss(y_train, clf.predict(X_train))\n",
    "        test_loss = log_loss(y_test, clf.predict(X_test))\n",
    "\n",
    "        empirical_accuracy = clf.score(X_train, y_train)\n",
    "        test_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "        empirical_losses.append(empirical_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        empirical_accuracies.append(empirical_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "    empirical_loss = np.mean(empirical_losses)\n",
    "    test_loss = np.mean(test_losses)\n",
    "\n",
    "    empirical_accuracy = np.mean(empirical_accuracies) * 100\n",
    "    test_accuracy = np.mean(test_accuracies) * 100\n",
    "\n",
    "    if print_accuracy:\n",
    "        print()\n",
    "        print(f\"empirical_accuracy: {empirical_accuracy: .2f}% | test_accuracy: {test_accuracy: .2f}% \")\n",
    "\n",
    "    return (empirical_loss, test_loss)\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "X_questions = data.iloc[:, 2:182] # Id e data de nascimento são irrelevantes\n",
    "X_questions = X_questions.drop('date_visit', axis=1) # Data de visita não é relevante\n",
    "X_questions = X_questions.drop(X_questions.columns[[46, 133, 158, 161]], axis=1) # Essas colunas são constantes\n",
    "\n",
    "X_drugs = data.iloc[:, 185:]\n",
    "X_drugs = X_drugs.drop(X_drugs.columns[[50,51,61,92,101,111,114,121,137,140,141,\n",
    "                                        142,143,148,151,152]], axis=1) # Essas colunas são constantes\n",
    "\n",
    "X_random = np.random.rand(X_questions.shape[0], 1) # Para comparar a perfomance do modelo\n",
    "\n",
    "# Codificando as variáveis categóricas\n",
    "le = LabelEncoder()\n",
    "for col in X_questions.columns:\n",
    "    if X_questions[col].dtype == 'bool':\n",
    "        X_questions[col] = le.fit_transform(X_questions[col])\n",
    "\n",
    "for col in X_drugs.columns:\n",
    "    if X_drugs[col].dtype == 'bool':\n",
    "        X_drugs[col] = le.fit_transform(X_drugs[col])\n",
    "\n",
    "# Imputando os valores que faltam\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "imp.fit(X_questions)\n",
    "X_questions = imp.transform(X_questions)\n",
    "\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "imp.fit(X_drugs)\n",
    "X_drugs = imp.transform(X_drugs)\n",
    "\n",
    "# Normalizando os dados\n",
    "scaler = StandardScaler()\n",
    "X_drugs = scaler.fit_transform(X_drugs)\n",
    "X_questions = scaler.fit_transform(X_questions)\n",
    "X_random = scaler.fit_transform(X_random)\n",
    "\n",
    "X_questions = pd.DataFrame(X_questions)\n",
    "X_drugs = pd.DataFrame(X_drugs)\n",
    "X_random = pd.DataFrame(X_random)\n",
    "\n",
    "Y = data.iloc[:, 182:185]\n",
    "\n",
    "y_vas30 = Y.iloc[:, 0:1].values.ravel()\n",
    "y_vas50 = Y.iloc[:, 1:2].values.ravel()\n",
    "y_gic = Y.iloc[:, 2:3].values.ravel()\n",
    "\n",
    "y_perceived = np.logical_and(y_vas30, y_vas50)\n",
    "y_perceived = y_perceived.astype(int)\n",
    "\n",
    "y = np.logical_and(y_perceived, y_gic)\n",
    "y = y.astype(int)\n",
    "y = pd.DataFrame(y)\n",
    "    \n",
    "one_rows = pd.DataFrame(y.index[(y == 1).all(axis=1)]) # Achando os indices das linhas que tem y=1\n",
    "zero_rows = pd.DataFrame( y.index[(y == 0).all(axis=1)]) # Achando os indices das linhas que tem y=0\n",
    "zero_rows = zero_rows.sample(n=one_rows.shape[0], random_state=1)\n",
    "print(zero_rows.index.tolist()) # selected indices\n",
    "\n",
    "# LABELED DATA\n",
    "X_questions_labeled = X_questions.iloc[one_rows[0].tolist() + zero_rows[0].tolist(), :].astype(int)\n",
    "X_drugs_labeled = X_drugs.iloc[one_rows[0].tolist() + zero_rows[0].tolist(), :].astype(int)\n",
    "X_labeled = np.concatenate((X_questions_labeled, X_drugs_labeled), axis=1).astype(int) # A junção das duas tabelas\n",
    "\n",
    "X_questions_labeled = pd.DataFrame(X_questions_labeled)\n",
    "X_drugs_labeled = pd.DataFrame(X_drugs_labeled)\n",
    "X_labeled = pd.DataFrame(X_labeled)\n",
    "\n",
    "y_labeled = y.iloc[one_rows[0].tolist() + zero_rows[0].tolist(), :].astype(int)\n",
    "y_labeled = pd.DataFrame(y_labeled)\n",
    "\n",
    "# UNLABELED DATA\n",
    "X_questions_unlabeled = X_questions.drop(index=X_questions_labeled.index)\n",
    "X_drugs_unlabeled = X_drugs.drop(index=X_drugs_labeled.index)\n",
    "X_unlabeled = np.concatenate((X_questions_unlabeled, X_drugs_unlabeled), axis=1) # A junção das duas tabelas\n",
    "y_unlabeled = y.drop(index=y_labeled.index)\n",
    "\n",
    "X_questions_unlabeled = pd.DataFrame(X_questions_unlabeled)\n",
    "X_drugs_unlabeled = pd.DataFrame(X_drugs_unlabeled)\n",
    "X_unlabeled = pd.DataFrame(X_unlabeled)\n",
    "y_unlabeled = pd.DataFrame(y_unlabeled)\n",
    "\n",
    "# SHUFFLE\n",
    "X_questions_labeled = X_questions_labeled.reset_index(drop=True)\n",
    "X_drugs_labeled = X_drugs_labeled.reset_index(drop=True)\n",
    "X_labeled = X_labeled.reset_index(drop=True)\n",
    "y_labeled = y_labeled.reset_index(drop=True)\n",
    "\n",
    "random_state = 42\n",
    "indices = y_labeled.index.tolist()\n",
    "shuffled_indices = pd.DataFrame(indices).sample(frac=1, random_state=random_state).values.ravel()\n",
    "X_questions_labeled = X_questions_labeled.iloc[shuffled_indices, :]\n",
    "X_drugs_labeled = X_drugs_labeled.iloc[shuffled_indices, :]\n",
    "X_labeled = X_labeled.iloc[shuffled_indices, :]\n",
    "y_labeled = y_labeled.iloc[shuffled_indices, :]\n",
    "\n",
    "# random_state = 42 # set a random state for reproducibility\n",
    "# indices = X_questions_labeled.index.tolist() # get the indices of the dataframes\n",
    "# shuffled_indices = pd.DataFrame(indices).sample(frac=1, random_state=random_state).values.ravel() # shuffle the indices\n",
    "\n",
    "# X_questions_labeled = X_questions_labeled.loc[shuffled_indices, :] # sort X_questions_labeled by the shuffled indices\n",
    "# X_drugs_labeled = X_drugs_labeled.loc[shuffled_indices, :] # sort X_drugs_labeled by the shuffled indices\n",
    "# X_labeled = X_labeled.loc[shuffled_indices, :] # sort X_labeled by the shuffled indices\n",
    "# y_labeled = y_labeled.loc[shuffled_indices, :] # sort y_labeled by the shuffled indices\n",
    "\n",
    "n_clusters = 2\n",
    "# model = AgglomerativeClustering(n_clusters=n_clusters, distance_threshold=None)\n",
    "# model.fit(X_unlabeled)\n",
    "\n",
    "# Z = linkage(X_unlabeled, method='ward')\n",
    "# plt.figure(figsize=(10, 5)) # plot the dendrogram\n",
    "# dendrogram(Z, truncate_mode='lastp', p=n_clusters)\n",
    "# # plt.show()\n",
    "\n",
    "# labels = model.labels_\n",
    "# unique_labels = np.unique(labels)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=100, max_iter=300)\n",
    "labels = kmeans.fit_predict(X_unlabeled)\n",
    "clusters = {}\n",
    "for i, label in enumerate(labels):\n",
    "    if label not in clusters:\n",
    "        clusters[label] = []\n",
    "    clusters[label].append(X_unlabeled.iloc[i:i+1, :])\n",
    "    # clusters[label] = clusters[label].append(X_unlabeled.iloc[i:i+1, :])\n",
    "    \n",
    "for key in clusters.keys():\n",
    "    clusters[key] = pd.concat(clusters[key])\n",
    "\n",
    "old_value0 = clusters[0]\n",
    "old_value1 = clusters[1]\n",
    "\n",
    "clusters[0] = old_value1\n",
    "clusters[1] = old_value0\n",
    "\n",
    "clf = xgb.XGBClassifier(max_depth=7,  gamma=0.7, eta=0.01, min_child_weight=1, subsample=0.8, \n",
    "                            colsample_bytree=0.8, scale_pos_weight=1)\n",
    "\n",
    "inices\n",
    "X_labeled = X_labeled.iloc[indices]\n",
    "clf = trainClassifier(X_labeled, y_labeled, clusters, clf, 5, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
