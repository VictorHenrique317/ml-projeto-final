{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repositório com o código desenvolvido para realizar o trabalho (https://github.com/VictorHenrique317/ml-projeto-final) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install xgboost\n",
    "# %pip install seaborn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from IPython.core.display import Image\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost as xgb\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo os diferentes conjuntos de features.\n",
    "- X_questions são as perguntas feitas para o paciente durante a consulta.\n",
    "- X_drugs são os remédios que o paciente toma.\n",
    "- X é a junção de X_Questions e X_drugs.\n",
    "- X_random é um conjunto de features aleatórias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdata.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m X_questions \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39miloc[:, \u001b[39m2\u001b[39m:\u001b[39m182\u001b[39m] \u001b[39m# Id e data de nascimento são irrelevantes\u001b[39;00m\n\u001b[0;32m      4\u001b[0m X_questions \u001b[39m=\u001b[39m X_questions\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mdate_visit\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# Data de visita não é relevante\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "X_questions = data.iloc[:, 2:182] # Id e data de nascimento são irrelevantes\n",
    "X_questions = X_questions.drop('date_visit', axis=1) # Data de visita não é relevante\n",
    "X_questions = X_questions.drop(X_questions.columns[[46, 133, 158, 161]], axis=1) # Essas colunas são constantes\n",
    "\n",
    "X_drugs = data.iloc[:, 185:]\n",
    "X_drugs = X_drugs.drop(X_drugs.columns[[50,51,61,92,101,111,114,121,137,140,141,\n",
    "                                        142,143,148,151,152]], axis=1) # Essas colunas são constantes\n",
    "\n",
    "X_random = np.random.rand(X_questions.shape[0], 1) # Para comparar a perfomance do modelo\n",
    "X = np.concatenate((X_questions, X_drugs), axis=1) # A junção das duas tabelas\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraindo as 3 diferentes variáveis que indicam se o paciente teve melhora ou não"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data.iloc[:, 182:185]\n",
    "\n",
    "y_vas30 = Y.iloc[:, 0:1].values.ravel()\n",
    "y_vas50 = Y.iloc[:, 1:2].values.ravel()\n",
    "y_gic = Y.iloc[:, 2:3].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse ponto temos 3 targets diferentes, temos que decidir qual faz mais sentido usar. Pensei que não faria sentido usar uma delas e ignorar o resto, então decidi criar uma nova variável que leva em consideração as 3 diferentes avaliações de melhora (2 do paciente e uma do médico).\n",
    "\n",
    "Primeiro criei y_perceived, que é a melhora percebida pelo paciente. Ela é definida como a disjunção entre y_vas30 e y_vas50 pois quero captar qualquer tipo de melhora percebida pelo paciente, seja ela pequena ou grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_perceived = np.logical_or(y_vas30, y_vas50)\n",
    "y_perceived = y_perceived.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E finalmente o target (y) é definido como a interseção entre melhora percebida e GIC, pois o paciente deve perceber alguma melhora e o médico deve concordar, esse é o cenário mais conservador possível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.logical_and(y_perceived, y_gic)\n",
    "y = y.astype(int)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porém, ao fazer isso a distribuição dos dados fica desbalanceada, apenas 15% dos exemplos são de pacientes que melhoraram segundo nossa nova variável y. \n",
    "\n",
    "Faz sentido a porcentagem de casos positivos ser baixa em y, pois como dito em nossa reunião a maior parte dos pacientes que sofrem com dor crônica não apresentam melhora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"A porcentagem de casos positivos em y_gic é {(np.sum(y_gic)/y_gic.shape[0])*100:.2f}%\")\n",
    "print(f\"A porcentagem de casos positivos em y_vas30 é {(np.sum(y_vas30)/y_vas30.shape[0])*100:.2f}%\")\n",
    "print(f\"A porcentagem de casos positivos em y_vas50 é {(np.sum(y_vas50)/y_vas50.shape[0])*100:.2f}%\")\n",
    "print()\n",
    "print(f\"A porcentagem de casos positivos em y_perceived é {(np.sum(y_perceived)/y_perceived.shape[0])*100:.2f}%\")\n",
    "print(f\"A porcentagem de casos positivos em y é {(np.sum(y.values)/y.shape[0])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então para tornar o modelo igualmente habilidoso tanto na predição de casos negativos, quanto na predição de casos positivos é necessário remover alguns casos negativos para que a distribuição dos dados seja mais equilibrada.\n",
    "\n",
    "Essa decisão tem um efeito adverso óbvio, o modelo terá uma menor qualidade devido a quantidade reduzida de dados. Porém,acredito que por se tratar de um modelo de grande responsábilidade (por atuar na área da saúde), ele deveria em tese identificar com a mesma confiabilidade tanto os casos positivos quanto os negativos para que não haja injustiças."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201, 312)\n",
      "(201, 175)\n",
      "(201, 137)\n",
      "(201,)\n"
     ]
    }
   ],
   "source": [
    "zero_rows = y.index[(y == 0).all(axis=1)] # Achando os indices das linhas que tem y=0\n",
    "# Selecionando aleatoriamente uma porcentagem dessas linhas para deletar\n",
    "delete_rows = np.random.choice(zero_rows, size=int(len(zero_rows)/1.25), replace=False)\n",
    "\n",
    "# Deletando as linhas selecionadas de todos os conjuntos de features\n",
    "X = X.drop(delete_rows)\n",
    "X_drugs = X_drugs.drop(delete_rows)\n",
    "X_questions = X_questions.drop(delete_rows)\n",
    "X_random = np.delete(X_random, delete_rows, axis=0)\n",
    "\n",
    "# Deletando as linhas selecionadas de todos os conjuntos targets\n",
    "y_gic = np.delete(y_gic, delete_rows)\n",
    "y_vas30 = np.delete(y_vas30, delete_rows)\n",
    "y_vas50 = np.delete(y_vas50, delete_rows)\n",
    "y_perceived = np.delete(y_perceived, delete_rows)\n",
    "y = np.delete(y, delete_rows)\n",
    "\n",
    "print(X.shape)\n",
    "print(X_questions.shape)\n",
    "print(X_drugs.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificando as variáveis categóricas\n",
    "le = LabelEncoder()\n",
    "for col in X_questions.columns:\n",
    "    if X_questions[col].dtype == 'bool':\n",
    "        X_questions[col] = le.fit_transform(X_questions[col])\n",
    "\n",
    "for col in X_drugs.columns:\n",
    "    if X_drugs[col].dtype == 'bool':\n",
    "        X_drugs[col] = le.fit_transform(X_drugs[col])\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'bool':\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# Imputando os valores que faltam\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "imp.fit(X_questions)\n",
    "X_questions = imp.transform(X_questions)\n",
    "\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "imp.fit(X_drugs)\n",
    "X_drugs = imp.transform(X_drugs)\n",
    "\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "\n",
    "# Normalizando os dados\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_drugs = scaler.fit_transform(X_drugs)\n",
    "X_questions = scaler.fit_transform(X_questions)\n",
    "X_random = scaler.fit_transform(X_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo as funções que irão treinar os diferentes algoritmos que selecionei. Os erros de teste são calculados usando Cross-Validation com 5 \"folds\" para serem uma melhor aproximação do erro esperado, as acuracias dos modelos também são registradas durante a validação cruzada.\n",
    "\n",
    "Selecionei dois algoritmos que não precisam de muitos dados o XGBoosting e a RandomForest, e pela curiosidade também treinei MLP's que naturalmente precisam de mais dados somente para comparar os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainClassifier(X, y, clf, print_accuracy=False):\n",
    "    kf = KFold(n_splits=5)\n",
    "    empirical_losses = []\n",
    "    test_losses = []\n",
    "    empirical_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for train_indices, test_indicies in kf.split(X):\n",
    "        X_train, X_test = X[train_indices], X[test_indicies]\n",
    "        y_train, y_test = y[train_indices], y[test_indicies]\n",
    "        \n",
    "        clf.fit(X_train, y_train) # classificador generico\n",
    "\n",
    "        empirical_loss = log_loss(y_train, clf.predict(X_train))\n",
    "        test_loss = log_loss(y_test, clf.predict(X_test))\n",
    "\n",
    "        empirical_accuracy = clf.score(X_train, y_train)\n",
    "        test_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "        empirical_losses.append(empirical_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        empirical_accuracies.append(empirical_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "    empirical_loss = np.mean(empirical_losses)\n",
    "    test_loss = np.mean(test_losses)\n",
    "\n",
    "    empirical_accuracy = np.mean(empirical_accuracies) * 100\n",
    "    test_accuracy = np.mean(test_accuracies) * 100\n",
    "\n",
    "    if print_accuracy:\n",
    "        print(f\"empirical_accuracy: {empirical_accuracy: .2f}% | test_accuracy: {test_accuracy: .2f}% \")\n",
    "\n",
    "    return (empirical_loss, test_loss)\n",
    "\n",
    "def trainXGBBoostingClassifier(X, y, max_depth=0, gamma=0.0, print_accuracy=False, print_importance=False):\n",
    "    clf = xgb.XGBClassifier(max_depth=max_depth,  gamma=gamma, eta=0.01, min_child_weight=1, subsample=0.8, \n",
    "                            colsample_bytree=0.8, scale_pos_weight=1)\n",
    "    \n",
    "    if print_importance:\n",
    "        clf.fit(X, y)\n",
    "        feat_imp = pd.Series(clf.get_booster().get_fscore())\n",
    "        feat_imp.index = pd.Index(feat_imp.index)\n",
    "        feat_imp.sort_values(ascending=False, inplace=True)\n",
    "        feat_imp.plot(kind='bar', title='Importância da feature', width=0.8, figsize=(20,10))\n",
    "        plt.ylabel('Avaliação de importância da feature')\n",
    "        \n",
    "    return trainClassifier(X, y, clf, print_accuracy)\n",
    "\n",
    "def trainMLPClassifier(X, y, hidden_layer_size=0, print_accuracy=False):\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(hidden_layer_size,), solver='sgd', learning_rate_init=0.01,\n",
    "                        max_iter=2000, verbose=False)\n",
    "    return trainClassifier(X, y, clf, print_accuracy)\n",
    "\n",
    "def trainRandomForestClassifier(X, y, max_depth=0, print_accuracy=False):\n",
    "    clf =clf = RandomForestClassifier(n_estimators=1000, max_depth=max_depth)\n",
    "    return trainClassifier(X, y, clf, print_accuracy)\n",
    "\n",
    "def createPlotDir(alg_name):\n",
    "    if not os.path.exists(f\"plots\"):\n",
    "        os.mkdir(f\"plots\")\n",
    "\n",
    "    if not os.path.exists(f\"plots/{alg_name}\"):\n",
    "        os.mkdir(f\"plots/{alg_name}/\")\n",
    "\n",
    "def savePlot(data, alg_name, filename):\n",
    "    x = sorted(data.keys())\n",
    "    empirical_losses = [data[key][0] for key in x]\n",
    "    test_losses = [data[key][1] for key in x]\n",
    "\n",
    "    legend = ['test loss', 'empirical loss']\n",
    "    plt.ylim((0,30))\n",
    "    plt.grid()\n",
    "    plt.plot(x, test_losses, color='blue', linestyle='dashed')\n",
    "    plt.plot(x, empirical_losses, color='blue')\n",
    "    plt.legend(legend)\n",
    "\n",
    "    plt.savefig(f'plots/{alg_name}/{filename}.png')\n",
    "    plt.clf() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora é a fase de seleção de modelos, para cada combinação (algoritmo, conjunto de features) plotei o gráfico de erro x capacidade para identificar o nível ideal de complexidade e o conjunto de features que é mais adequado para a classificação.\n",
    "\n",
    "- A medida de complexidade para as MLP's de 3 camadas é o número de neurônios na camada oculta.\n",
    "- A medida de complexidade para as Random Forests é a profundidade máxima das árvores (classificadores individuais).\n",
    "- A medida de complexidade para o XGBoost é a profundidade máxima das árvores (classificadores individuais)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCapacityGraphsForMLP(X, y, filename, max_neuron_nb):\n",
    "    createPlotDir('mlp')\n",
    "\n",
    "    data = dict()\n",
    "    for neuron_nb in range(1, max_neuron_nb+1, 10):\n",
    "        print(f\"{(neuron_nb/max_neuron_nb)*100:.2f}%...\", end=\"\\r\")\n",
    "        (empirical_loss, test_loss) = trainMLPClassifier(X, y, hidden_layer_size=neuron_nb)\n",
    "        data[neuron_nb] = (empirical_loss, test_loss)\n",
    "   \n",
    "    savePlot(data, 'mlp', filename)\n",
    "\n",
    "max_neuron_nb = 200\n",
    "plotCapacityGraphsForMLP(X, y, \"x\", max_neuron_nb)\n",
    "plotCapacityGraphsForMLP(X_questions, y, \"x_questions\", max_neuron_nb)\n",
    "plotCapacityGraphsForMLP(X_drugs, y, \"x_drugs\", max_neuron_nb)\n",
    "plotCapacityGraphsForMLP(X_random, y, \"x_random\", max_neuron_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCapacityGraphsForXGBoost(X, y, filename, max_depth, print_accuracy=False):\n",
    "    gamma = 0\n",
    "    createPlotDir('xgboost')\n",
    "\n",
    "    data = dict()\n",
    "    for depth in range(1, max_depth+1):\n",
    "        print(f\"{(depth/max_depth)*100:.2f}%...\", end=\"\\r\")\n",
    "        (empirical_loss, test_loss) = trainXGBBoostingClassifier(X, y, max_depth=depth, gamma=gamma, print_accuracy=print_accuracy)\n",
    "        data[depth] = (empirical_loss, test_loss)\n",
    "   \n",
    "    savePlot(data, 'xgboost', filename)\n",
    "\n",
    "max_depth = 10\n",
    "plotCapacityGraphsForXGBoost(X, y, \"x\", max_depth) # Adicionar X_drugs parece não ter efeito\n",
    "plotCapacityGraphsForXGBoost(X_questions, y, \"x_questions\", max_depth, print_accuracy=False) # Resultado diferente do aleatorio mas ainda sim\n",
    "plotCapacityGraphsForXGBoost(X_drugs, y_gic, \"x_drugs\", max_depth) # X_drugs parece ter nenhum poder preditivo\n",
    "plotCapacityGraphsForXGBoost(X_random, y, \"x_random\", max_depth, print_accuracy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCapacityGraphsForRandomForest(X, y, filename, max_depth):\n",
    "    gamma = 0\n",
    "    createPlotDir('random_forest')\n",
    "\n",
    "    data = dict()\n",
    "    for depth in range(1, max_depth+1):\n",
    "        print(f\"{(depth/max_depth)*100:.2f}%...\", end=\"\\r\")\n",
    "        (empirical_loss, test_loss) = trainRandomForestClassifier(X, y, max_depth=depth)\n",
    "        data[depth] = (empirical_loss, test_loss)\n",
    "   \n",
    "    savePlot(data, 'random_forest', filename)\n",
    "\n",
    "max_depth = 20\n",
    "plotCapacityGraphsForRandomForest(X, y, \"x\", max_depth)\n",
    "plotCapacityGraphsForRandomForest(X_questions, y, \"x_questions\", max_depth)\n",
    "plotCapacityGraphsForRandomForest(X_drugs, y, \"x_drugs\", max_depth)\n",
    "plotCapacityGraphsForRandomForest(X_random, y, \"x_random\", max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecionei o XGBoost pois ele tem resultados ligeramente melhores (menor erro de teste e maior acuracia) que os da random forest, e alem disso ele tem mais hiperparâmetros para ajustar a qualidade do modelo. Como esperado o desempenho da MLP é baixo e não consistente ao longo dos diferentes níveis de complexidade.\n",
    "\n",
    "O ajuste de hiperparâmetros nesse contexto não é muito relevante, já que a qualidade geral dos modelos é bem baixa devido ao pequeno número de dados.\n",
    "\n",
    "Comparei os diferentes modelos primeiro considerando apenas o conjunto de features X, depois de selecionado o modelo podemos ver como os diferentes conjuntos de features afetam a capacidade preditiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = trainRandomForestClassifier(X, y, max_depth=3, print_accuracy=True)\n",
    "_ =trainRandomForestClassifier(X_random, y, max_depth=3, print_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1618\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1617\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1618\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_take_with_is_copy(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   1619\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m   1620\u001b[0m     \u001b[39m# re-raise with different error message\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:3948\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   3941\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3942\u001b[0m \u001b[39mInternal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   3943\u001b[0m \u001b[39mattribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3946\u001b[0m \u001b[39mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   3947\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3948\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take(indices\u001b[39m=\u001b[39;49mindices, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   3949\u001b[0m \u001b[39m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:3932\u001b[0m, in \u001b[0;36mNDFrame._take\u001b[1;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[0;32m   3930\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 3932\u001b[0m new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mtake(\n\u001b[0;32m   3933\u001b[0m     indices,\n\u001b[0;32m   3934\u001b[0m     axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_block_manager_axis(axis),\n\u001b[0;32m   3935\u001b[0m     verify\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   3936\u001b[0m     convert_indices\u001b[39m=\u001b[39;49mconvert_indices,\n\u001b[0;32m   3937\u001b[0m )\n\u001b[0;32m   3938\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtake\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:960\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify, convert_indices)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[39mif\u001b[39;00m convert_indices:\n\u001b[1;32m--> 960\u001b[0m     indexer \u001b[39m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[39m=\u001b[39;49mverify)\n\u001b[0;32m    962\u001b[0m new_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\u001b[39m.\u001b[39mtake(indexer)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexers\\utils.py:284\u001b[0m, in \u001b[0;36mmaybe_convert_indices\u001b[1;34m(indices, n, verify)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[1;32m--> 284\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mindices are out-of-bounds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m indices\n",
      "\u001b[1;31mIndexError\u001b[0m: indices are out-of-bounds",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m _ \u001b[39m=\u001b[39m trainXGBBoostingClassifier(X, y, max_depth\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, gamma\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m, print_accuracy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, print_importance\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      2\u001b[0m _ \u001b[39m=\u001b[39m trainXGBBoostingClassifier(X_random, y, max_depth\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, print_accuracy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[30], line 49\u001b[0m, in \u001b[0;36mtrainXGBBoostingClassifier\u001b[1;34m(X, y, max_depth, gamma, print_accuracy, print_importance)\u001b[0m\n\u001b[0;32m     46\u001b[0m     feat_imp\u001b[39m.\u001b[39mplot(kind\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbar\u001b[39m\u001b[39m'\u001b[39m, title\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mImportância da feature\u001b[39m\u001b[39m'\u001b[39m, width\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m20\u001b[39m,\u001b[39m10\u001b[39m))\n\u001b[0;32m     47\u001b[0m     plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mAvaliação de importância da feature\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m \u001b[39mreturn\u001b[39;00m trainClassifier(X, y, clf, print_accuracy)\n",
      "Cell \u001b[1;32mIn[30], line 9\u001b[0m, in \u001b[0;36mtrainClassifier\u001b[1;34m(X, y, clf, print_accuracy)\u001b[0m\n\u001b[0;32m      6\u001b[0m test_accuracies \u001b[39m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m train_indices, test_indicies \u001b[39min\u001b[39;00m kf\u001b[39m.\u001b[39msplit(X):\n\u001b[1;32m----> 9\u001b[0m     X_train, X_test \u001b[39m=\u001b[39m X_labeled\u001b[39m.\u001b[39;49miloc[train_indices], X_labeled\u001b[39m.\u001b[39miloc[test_indicies]\n\u001b[0;32m     10\u001b[0m     y_train, y_test \u001b[39m=\u001b[39m y_labeled\u001b[39m.\u001b[39miloc[train_indices], y_labeled\u001b[39m.\u001b[39miloc[test_indicies]\n\u001b[0;32m     12\u001b[0m     clf\u001b[39m.\u001b[39mfit(X_train, y_train) \u001b[39m# classificador generico\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1100\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m   1102\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m-> 1103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1647\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1645\u001b[0m \u001b[39m# a list of integers\u001b[39;00m\n\u001b[0;32m   1646\u001b[0m \u001b[39melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[1;32m-> 1647\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_list_axis(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   1649\u001b[0m \u001b[39m# a single integer\u001b[39;00m\n\u001b[0;32m   1650\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1651\u001b[0m     key \u001b[39m=\u001b[39m item_from_zerodim(key)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1621\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1618\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_take_with_is_copy(key, axis\u001b[39m=\u001b[39maxis)\n\u001b[0;32m   1619\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m   1620\u001b[0m     \u001b[39m# re-raise with different error message\u001b[39;00m\n\u001b[1;32m-> 1621\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpositional indexers are out-of-bounds\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "_ = trainXGBBoostingClassifier(X, y, max_depth=3, gamma=0.7, print_accuracy=True, print_importance=False)\n",
    "_ = trainXGBBoostingClassifier(X_random, y, max_depth=3, gamma=0.7, print_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='plots/manual/models_capacity_x.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que selecionamos o melhor modelo podemos analisar como os diferentes conjuntos de features afetam a capacidade preditiva. Para fins comparação gerei X_random, um conjunto de features aleatórias que nos permite ver o quão menor o erro de teste de cada conjunto está em relação ao erro de teste gerado a partir de valores aleatórios.\n",
    "\n",
    "X_questions e X_drugs geram um desempenho semelhante, porém x_questions gera resultados levemente melhores. Quando usamos todas a as features disponíveis (X) temos o melhor modelo por uma faixa bem pequena.\n",
    "\n",
    "Com esses resultados não conseguimos afirmar com certeza se os remédios que o paciente toma podem ser usados para predizer se ele vai ter uma melhora em sua dor crônica ou não. Isso porque o melhor desempenho do modelo que usa o conjunto X pode ser explicado tanto pelo presença das features de remédios quanto pelo aumento da dimensionalidade dos dados. Lembrando que quanto maior a dimensionalidade dos dados maior a chance do modelo ser linearmente separável, e por consequência ter um melhor desempenho.\n",
    "\n",
    "Porém, não podemos esquecer do fato que o modelo que usa X_drugs é tão bom quanto o que usa X_questions. Isso pode ser um indício que os remédios tem sim alguma capacidade preditiva, já que já foi comprovado que X_questions pode ser usado para fazer essa predição (como discutido em nossa reunião)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='plots/manual/xgboost_features.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fazer uma última análise do modelo obtido usaremos o princípio da navalha de Ockham para escolher o conjunto de features X_questions, já que ele é menos complexo que X e gera um modelo com aproximadamente a mesma qualidade.\n",
    "\n",
    "Podemos observar que pela confusion matrix dos dados originais (com uma distribuição assimétrica) que o modelo é completamente desbalanceado, só sabendo identificar verdadeiros negativos e tendo uma péssima acurácia para verdadeiros positivos. Enquanto o outro, apesar de ter piores resultados, é mais balanceado em suas classificações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanConfusionMatrix(X, y, name):\n",
    "    k = 40\n",
    "    mean_confusion_matrix = np.zeros((2,2))\n",
    "    kfold = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        clf = xgb.XGBClassifier(max_depth=3,  gamma=0.7, eta=0.01, min_child_weight=1, subsample=0.8, \n",
    "                            colsample_bytree=0.8, scale_pos_weight=1)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        mean_confusion_matrix += confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    mean_confusion_matrix /= k\n",
    "    sns.set(font_scale=1.4)\n",
    "    sns.heatmap(mean_confusion_matrix, annot=True, annot_kws={\"size\": 16}, cmap='Blues', fmt='.2g')\n",
    "\n",
    "    plt.title(name)\n",
    "    plt.xlabel('Categoria predita')\n",
    "    plt.ylabel('Categoria real')\n",
    "    plt.show()\n",
    "\n",
    "meanConfusionMatrix(X_questions, y, 'Confusion matrix usando X_questions bem distribuido')\n",
    "Image(filename='plots/manual/confusion_matrix_original_data.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vejamos quais são as features mais importantes para o classificador XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "questions = df.iloc[:, 2:182]\n",
    "questions = questions.drop('date_visit', axis=1)\n",
    "questions = questions.drop(questions.columns[[46, 133, 158, 161]], axis=1)\n",
    "\n",
    "drugs = data.iloc[:, 185:]\n",
    "drugs = drugs.drop(drugs.columns[[50,51,61,92,101,111,114,121,137,140,141,\n",
    "                                        142,143,148,151,152]], axis=1)\n",
    "\n",
    "features = pd.concat([questions, drugs], axis=1)\n",
    "features = list(features.columns)\n",
    "\n",
    "clf = xgb.XGBClassifier(max_depth=3,  gamma=0.7, eta=0.01, min_child_weight=1, subsample=0.8, \n",
    "                            colsample_bytree=0.8, scale_pos_weight=1)\n",
    "clf.fit(X_questions, y)\n",
    "\n",
    "feat_imp = pd.Series(clf.get_booster().get_fscore())\n",
    "feat_imp.index = pd.Index(feat_imp.index)\n",
    "feat_imp.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "x_plot = [ int(f.replace(\"f\", \"\")) for f in feat_imp.index]\n",
    "y_plot = [ feat_imp[f] for f in feat_imp.index]\n",
    "x_plot = [features[feature_index] for feature_index in x_plot]\n",
    "\n",
    "max_features = 40\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.bar(x_plot[:max_features], y_plot[:max_features], label=\"Importância da feature\")\n",
    "ax.set_ylabel(\"Avaliação de importância da feature\")\n",
    "plt.xticks(rotation=90)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusão: Mais dados são necessários para afirmar responsavelmente se os remédios predizem (ou não) a melhora dos pacientes, principalmente dados de casos positivos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
